

from prompts.llm_as_judge import get_pairwise_judge_react_prompt

# We further evaluate these models using our process supervision labels, aiming to assess each modelâ€™s
# effectiveness as a pairwise judge in selecting the human-corrected step over the step generated by the
# original policy used during annotation. To mitigate position bias, we swap the order of the human-
# corrected and model-generated steps and conduct two separate predictions for each arrangement.
# Additionally, models are permitted to indicate a tie. If a model designates a tie at least once, or
# consistently predicts the same position (before and after swapping) for a given data sample, we
# classify the outcome as a tie. Mirroring the methodology used in RewardBench (Lambert et al.
# (2024)), we score losses as 0, ties as 0.5, and wins as 1.

import argparse
import ast
import datetime
import json
import os
import re
from typing import Any, Dict, List, Tuple
from tqdm import tqdm
import traceback
import litellm
from utils.keystore import auth_litellm
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import random

def _safe_json_loads(line: str) -> Dict[str, Any]:
    """Parse a line from the dataset that may contain NaN and non-JSON values.

    - Replace occurrences of ': NaN' with ': null'
    - Fall back to eval if necessary (shouldn't be needed)
    """
    # Replace unquoted NaN tokens with null for JSON compatibility
    return json.loads(line)

    # fixed = re.sub(r":\s*NaN(,|})", r": null\1", line)
    # try:
    #     return json.loads(fixed)
    # except json.JSONDecodeError:
    #     # As a last resort, try to coerce single quotes to double quotes for top-level keys
    #     try:
    #         coerced = fixed.replace("'", '"')
    #         return json.loads(coerced)
    #     except Exception as e:
    #         raise e


def _ensure_list_tools(entry: Dict[str, Any]) -> None:
    tools = entry.get("tools")
    if isinstance(tools, str):
        # Often stored as a Python list string, e.g. "['a','b']"
        try:
            parsed = ast.literal_eval(tools)
            if isinstance(parsed, list):
                entry["tools"] = parsed
            else:
                entry["tools"] = []
        except Exception:
            entry["tools"] = []


def _extract_label(text: str, is_plan: bool) -> Tuple[str, Dict[str, Any]]:
    """Extract the better_step or better_action_plan label from model output.

    Returns (label_value, debug_dict)
    label_value in {"1", "2", "tie", "unknown"}
    """
    debug: Dict[str, Any] = {"raw": text}

    # Try to find a JSON fenced block first
    fence_match = re.search(r"```json\s*\{[^`]*\}\s*```", text, flags=re.IGNORECASE | re.DOTALL)
    json_str = None
    if fence_match:
        json_str = fence_match.group(0)
        json_str = re.sub(r"^```json", "", json_str, flags=re.IGNORECASE).strip()
        json_str = re.sub(r"```$", "", json_str).strip()
    else:
        # Try to extract minimal dict substring
        curly = re.search(r"\{[^\}]*\}", text, flags=re.DOTALL)
        if curly:
            json_str = curly.group(0)

    if json_str:
        try:
            data = json.loads(json_str)
            debug["parsed"] = data
            val = data.get("better_step") if not is_plan else data.get("better_action_plan")
            if isinstance(val, str) and val.lower() in {"tie", "1", "2"}:
                return val.lower(), debug
            if val in (1, 2):
                return str(val), debug
        except Exception as e:
            debug["json_error"] = str(e)

    # Regex fallback
    m = re.search(r"better_step\s*[:=]\s*\"?(tie|1|2)\"?", text, flags=re.IGNORECASE) if not is_plan else re.search(r"better_action_plan\s*[:=]\s*\"?(tie|1|2)\"?", text, flags=re.IGNORECASE)
    if m:
        return m.group(1).lower(), debug

    return "unknown", debug


def _score_pair(l1: str, l2: str) -> Tuple[float, str]:
    """Score a sample based on two predictions.

    - If either is tie -> tie (0.5)
    - If predictions are (1,2) => win (1.0)
    - If predictions are (2,1) => loss (0.0)
    - If both same numeric (1,1) or (2,2) => tie (0.5) due to position bias
    - Else unknown => tie (0.5) conservative
    """
    if l1 == "tie" or l2 == "tie" or l1 == "unknown" or l2 == "unknown":
        return 0.5, "tie"
    if l1 == "1" and l2 == "2":
        return 1.0, "win"
    if l1 == "2" and l2 == "1":
        return 0.0, "loss"
    if l1 == l2:
        return 0.5, "tie"
    return 0.5, "tie"


def _history_is_empty(entry: Dict[str, Any]) -> bool:
    hist = json.loads(entry.get("history"))
    if hist is None:
        return True
    if isinstance(hist, str):
        return len(hist.strip()) == 0
    if isinstance(hist, list):
        return len(hist) == 0
    return not bool(hist)


def load_dataset(path: str, limit: int = None) -> List[Dict[str, Any]]:
    items: List[Dict[str, Any]] = []
    with open(path, "r", encoding="utf-8") as f:
        for idx, line in enumerate(f):
            line = line.strip()
            if not line:
                continue
            try:
                obj = _safe_json_loads(line)
                _ensure_list_tools(obj)
                items.append(obj)
            except Exception:
                # Skip malformed lines but continue
                continue
            if limit is not None and len(items) >= limit:
                break
    return items


def run_inference(
    dataset_path: str,
    out_log_path: str,
    max_samples: int = None,
    max_workers: int = 8,
    sampling_config: Dict[str, Any] = None,
) -> Dict[str, Any]:
    data = load_dataset(dataset_path, limit=max_samples)

    os.makedirs(os.path.dirname(out_log_path), exist_ok=True)
    log_f = open(out_log_path, "w", encoding="utf-8")

    total_score = 0.0
    total_count = 0

    plan_score = 0.0
    plan_count = 0

    react_score = 0.0
    react_count = 0

    # Configure API client once for all threads
    api_key, api_base = auth_litellm()
    litellm.api_base = api_base
    litellm.api_key = api_key

    def process_one(i: int, entry: Dict[str, Any]) -> Dict[str, Any]:
        try:
            # Normalize fields expected by the prompt builder
            if "history" not in entry or entry["history"] is None:
                entry["history"] = ""
            elif isinstance(entry["history"], (list, dict)):
                entry["history"] = json.dumps(entry["history"], ensure_ascii=False)

            try:
                preferred_first, dispreferred_first = get_pairwise_judge_react_prompt(entry)
            except Exception:
                return {
                    "valid": False,
                    "log": {
                        "index": i,
                        "error": f"prompt_build_failed: {traceback.format_exc()}",
                        "entry_keys": list(entry.keys()),
                    },
                }

            max_retries = 10
            base_delay = 15
            retry_count_rate_limit = 0

            
            use_responses_api = "o3" or "o1" in sampling_config["model"]
            for i in range(max_retries):
                try:
                    if use_responses_api:
                        # use responses api
                        out1 = litellm.responses(
                            input=preferred_first,
                            **sampling_config
                        )
                        out2 = litellm.responses(
                            input=dispreferred_first,
                            **sampling_config
                        )
                        break
                    else:
                        out1 = litellm.completion(
                            messages=[{"role": "user", "content": preferred_first}],
                            **sampling_config
                            
                        )
                        out2 = litellm.completion(
                            messages=[{"role": "user", "content": dispreferred_first}],
                            **sampling_config
                        )
                        break
                except Exception:
                    # exponential backoff
                    delay = base_delay * (2 ** (retry_count_rate_limit - 1))  # exponential increase
                    delay = delay * (0.5 + random.random())  # add jitter (50-150% of delay)
                    retry_count_rate_limit += 1
                    if i == max_retries - 1:
                        return {
                            "valid": False,
                            "log": {
                                "index": i,
                                "error": f"request_failed: {traceback.format_exc()}",
                                "entry_keys": list(entry.keys()),
                            },
                        }
                    time.sleep(delay)

            if use_responses_api:
                pred1_raw = out1.output[-1].content[0].text
                pred2_raw = out2.output[-1].content[0].text
            else:
                pred1_raw = out1.choices[0].message.content
                pred2_raw = out2.choices[0].message.content

            print(pred1_raw)
            print(pred2_raw)

            is_plan = _history_is_empty(entry)
            label1, dbg1 = _extract_label(pred1_raw, is_plan)
            label2, dbg2 = _extract_label(pred2_raw, is_plan)
            score, decision = _score_pair(label1, label2)

            log_obj = {
                "index": i,
                "prompt": entry.get("prompt"),
                "history_is_empty": is_plan,
                "preferred_first_prompt": preferred_first,
                "dispreferred_first_prompt": dispreferred_first,
                "prediction_preferred_first": {"raw": pred1_raw, "label": label1, "debug": dbg1},
                "prediction_dispreferred_first": {"raw": pred2_raw, "label": label2, "debug": dbg2},
                "final": {"decision": decision, "score": score},
            }

            return {
                "valid": True,
                "score": score,
                "is_plan": is_plan,
                "log": log_obj,
            }
        except Exception:
            return {
                "valid": False,
                "log": {
                    "index": i,
                    "error": f"unexpected_error: {traceback.format_exc()}",
                    "entry_keys": list(entry.keys()),
                },
            }

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_one, i, entry) for i, entry in enumerate(data)]
        for fut in tqdm(as_completed(futures), total=len(futures), desc="Processing samples"):
            res = fut.result()
            log_f.write(json.dumps(res["log"], ensure_ascii=False) + "\n")
            log_f.flush()

            if res.get("valid"):
                score = res["score"]
                is_plan = res["is_plan"]

                total_score += score
                total_count += 1

                if is_plan:
                    plan_score += score
                    plan_count += 1
                else:
                    react_score += score
                    react_count += 1

    log_f.close()

    def ratio(s: float, n: int) -> float:
        return (s / n) if n > 0 else 0.0

    return {
        "total_accuracy": ratio(total_score, total_count),
        "num_samples": total_count,
        "action_plan_only_accuracy": ratio(plan_score, plan_count),
        "action_plan_only_count": plan_count,
        "react_steps_accuracy": ratio(react_score, react_count),
        "react_steps_count": react_count,
        "log_path": out_log_path,
    }


def main():
    parser = argparse.ArgumentParser(description="Run pairwise LLM-as-judge evaluation with process supervision labels.")
    parser.add_argument(
        "--dataset",
        type=str,
        default="/mnt/efs/vaskarnath/workspace/toolcomp/toolcomp_process_supervision_data.jsonl",
        help="Path to the jsonl dataset",
    )
    parser.add_argument(
        "--config_file",
        type=str,
        help="The config file to use for generation",
        required=True,
    )
    parser.add_argument("--out", type=str, default=None, help="Output log path (jsonl). If omitted, auto-generated.")
    parser.add_argument("--limit", type=int, default=None, help="Limit the number of samples")
    parser.add_argument("--max_workers", type=int, default=8, help="Number of concurrent worker threads")

    args = parser.parse_args()

    with open(args.config_file) as f:
        sampling_config = json.load(f)

    os.makedirs(args.out, exist_ok=True)

    # save full config file
    with open(os.path.join(args.out, "config.json"), "w") as f:
        json.dump({"args": args.__dict__, "sampling_config": sampling_config}, f)

    if args.out is None:
        ts = datetime.datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        out_log_path = f"/mnt/efs/vaskarnath/workspace/toolcomp/inference/logs/llm_as_judge_{args.config_file.replace('/', '_')}_{ts}.jsonl"
    else:
        # add llm_as_judge_ to the out path
        out_log_path = os.path.join(args.out, "llm_as_judge.jsonl")

    results = run_inference(
        dataset_path=args.dataset,
        out_log_path=out_log_path,
        max_samples=args.limit,
        max_workers=args.max_workers,
        sampling_config=sampling_config
    )

    # Print concise metrics
    print(json.dumps(results, indent=2))
    # save metrics to a json file
    with open(os.path.join(args.out, "metrics.json"), "w") as f:
        json.dump(results, f)


if __name__ == "__main__":
    main()
